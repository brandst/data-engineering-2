{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c13dbc0-6959-44d7-8ddd-ef3536521252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- from_json(value): struct (nullable = true)\n",
      " |    |-- Review_ID: string (nullable = true)\n",
      " |    |-- Movie_ID: string (nullable = true)\n",
      " |    |-- Reviewer_Name: string (nullable = true)\n",
      " |    |-- Review_Rating: integer (nullable = true)\n",
      " |    |-- Reviewer_Nationality: string (nullable = true)\n",
      " |    |-- Reviewer_Age: integer (nullable = true)\n",
      " |    |-- Review_Date: string (nullable = true)\n",
      " |    |-- Sex: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Review_ID: string (nullable = true)\n",
      " |-- Movie_ID: string (nullable = true)\n",
      " |-- Reviewer_Name: string (nullable = true)\n",
      " |-- Review_Rating: integer (nullable = true)\n",
      " |-- Reviewer_Nationality: string (nullable = true)\n",
      " |-- Reviewer_Age: integer (nullable = true)\n",
      " |-- Review_Date: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\nGlobalLimit 10\n+- LocalLimit 10\n   +- Sort [Movie_Average_Rating#262 DESC NULLS LAST], true\n      +- Aggregate [Movie_ID#198], [Movie_ID#198, avg(Review_Rating#200) AS Movie_Average_Rating#262]\n         +- Join Inner, (Reviewer_Nationality#201 = Reviewer_Nationality#230)\n            :- Project [from_json(value)#195.Review_ID AS Review_ID#197, from_json(value)#195.Movie_ID AS Movie_ID#198, from_json(value)#195.Reviewer_Name AS Reviewer_Name#199, from_json(value)#195.Review_Rating AS Review_Rating#200, from_json(value)#195.Reviewer_Nationality AS Reviewer_Nationality#201, from_json(value)#195.Reviewer_Age AS Reviewer_Age#202, from_json(value)#195.Review_Date AS Review_Date#203, from_json(value)#195.Sex AS Sex#204]\n            :  +- Project [from_json(StructField(Review_ID,StringType,true), StructField(Movie_ID,StringType,true), StructField(Reviewer_Name,StringType,true), StructField(Review_Rating,IntegerType,true), StructField(Reviewer_Nationality,StringType,true), StructField(Reviewer_Age,IntegerType,true), StructField(Review_Date,StringType,true), StructField(Sex,StringType,true), value#193, Some(Etc/UTC)) AS from_json(value)#195]\n            :     +- Project [cast(value#180 as string) AS value#193]\n            :        +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@1aebc2b3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@27cf614b, [startingOffsets=latest, kafka.bootstrap.servers=kafka1:9093, subscribe=reviews], [key#179, value#180, topic#181, partition#182, offset#183L, timestamp#184, timestampType#185], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@59d8c1ad,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:9093, subscribe -> reviews, startingOffsets -> latest),None), kafka, [key#172, value#173, topic#174, partition#175, offset#176L, timestamp#177, timestampType#178]\n            +- SubqueryAlias Lowest_Nationality\n               +- Project [Reviewer_Nationality#230]\n                  +- GlobalLimit 1\n                     +- LocalLimit 1\n                        +- Sort [Average_Rating#222 ASC NULLS FIRST], true\n                           +- Aggregate [Reviewer_Nationality#230], [Reviewer_Nationality#230, avg(Review_Rating#229) AS Average_Rating#222]\n                              +- Project [from_json(value)#195.Review_ID AS Review_ID#226, from_json(value)#195.Movie_ID AS Movie_ID#227, from_json(value)#195.Reviewer_Name AS Reviewer_Name#228, from_json(value)#195.Review_Rating AS Review_Rating#229, from_json(value)#195.Reviewer_Nationality AS Reviewer_Nationality#230, from_json(value)#195.Reviewer_Age AS Reviewer_Age#231, from_json(value)#195.Review_Date AS Review_Date#232, from_json(value)#195.Sex AS Sex#233]\n                                 +- Project [from_json(StructField(Review_ID,StringType,true), StructField(Movie_ID,StringType,true), StructField(Reviewer_Name,StringType,true), StructField(Review_Rating,IntegerType,true), StructField(Reviewer_Nationality,StringType,true), StructField(Reviewer_Age,IntegerType,true), StructField(Review_Date,StringType,true), StructField(Sex,StringType,true), value#193, Some(Etc/UTC)) AS from_json(value)#195]\n                                    +- Project [cast(value#180 as string) AS value#193]\n                                       +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@1aebc2b3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@27cf614b, [startingOffsets=latest, kafka.bootstrap.servers=kafka1:9093, subscribe=reviews], [key#179, value#180, topic#181, partition#182, offset#183L, timestamp#184, timestampType#185], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@59d8c1ad,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:9093, subscribe -> reviews, startingOffsets -> latest),None), kafka, [key#172, value#173, topic#174, partition#175, offset#176L, timestamp#177, timestampType#178]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 108\u001b[0m\n\u001b[1;32m     96\u001b[0m     df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbigquery\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m     97\u001b[0m       \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataengineering-439112.labdataset.best_rated_by_critic_nationality\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m     98\u001b[0m       \u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     99\u001b[0m       \u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Output to the console or sink\u001b[39;00m\n\u001b[1;32m    102\u001b[0m query \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    103\u001b[0m     \u001b[43mtop_movies_for_lowest_nationality\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessingTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2 seconds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeachBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_foreach_batch_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 108\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m )\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     query\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/readwriter.py:1527\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\nGlobalLimit 10\n+- LocalLimit 10\n   +- Sort [Movie_Average_Rating#262 DESC NULLS LAST], true\n      +- Aggregate [Movie_ID#198], [Movie_ID#198, avg(Review_Rating#200) AS Movie_Average_Rating#262]\n         +- Join Inner, (Reviewer_Nationality#201 = Reviewer_Nationality#230)\n            :- Project [from_json(value)#195.Review_ID AS Review_ID#197, from_json(value)#195.Movie_ID AS Movie_ID#198, from_json(value)#195.Reviewer_Name AS Reviewer_Name#199, from_json(value)#195.Review_Rating AS Review_Rating#200, from_json(value)#195.Reviewer_Nationality AS Reviewer_Nationality#201, from_json(value)#195.Reviewer_Age AS Reviewer_Age#202, from_json(value)#195.Review_Date AS Review_Date#203, from_json(value)#195.Sex AS Sex#204]\n            :  +- Project [from_json(StructField(Review_ID,StringType,true), StructField(Movie_ID,StringType,true), StructField(Reviewer_Name,StringType,true), StructField(Review_Rating,IntegerType,true), StructField(Reviewer_Nationality,StringType,true), StructField(Reviewer_Age,IntegerType,true), StructField(Review_Date,StringType,true), StructField(Sex,StringType,true), value#193, Some(Etc/UTC)) AS from_json(value)#195]\n            :     +- Project [cast(value#180 as string) AS value#193]\n            :        +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@1aebc2b3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@27cf614b, [startingOffsets=latest, kafka.bootstrap.servers=kafka1:9093, subscribe=reviews], [key#179, value#180, topic#181, partition#182, offset#183L, timestamp#184, timestampType#185], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@59d8c1ad,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:9093, subscribe -> reviews, startingOffsets -> latest),None), kafka, [key#172, value#173, topic#174, partition#175, offset#176L, timestamp#177, timestampType#178]\n            +- SubqueryAlias Lowest_Nationality\n               +- Project [Reviewer_Nationality#230]\n                  +- GlobalLimit 1\n                     +- LocalLimit 1\n                        +- Sort [Average_Rating#222 ASC NULLS FIRST], true\n                           +- Aggregate [Reviewer_Nationality#230], [Reviewer_Nationality#230, avg(Review_Rating#229) AS Average_Rating#222]\n                              +- Project [from_json(value)#195.Review_ID AS Review_ID#226, from_json(value)#195.Movie_ID AS Movie_ID#227, from_json(value)#195.Reviewer_Name AS Reviewer_Name#228, from_json(value)#195.Review_Rating AS Review_Rating#229, from_json(value)#195.Reviewer_Nationality AS Reviewer_Nationality#230, from_json(value)#195.Reviewer_Age AS Reviewer_Age#231, from_json(value)#195.Review_Date AS Review_Date#232, from_json(value)#195.Sex AS Sex#233]\n                                 +- Project [from_json(StructField(Review_ID,StringType,true), StructField(Movie_ID,StringType,true), StructField(Reviewer_Name,StringType,true), StructField(Review_Rating,IntegerType,true), StructField(Reviewer_Nationality,StringType,true), StructField(Reviewer_Age,IntegerType,true), StructField(Review_Date,StringType,true), StructField(Sex,StringType,true), value#193, Some(Etc/UTC)) AS from_json(value)#195]\n                                    +- Project [cast(value#180 as string) AS value#193]\n                                       +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@1aebc2b3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@27cf614b, [startingOffsets=latest, kafka.bootstrap.servers=kafka1:9093, subscribe=reviews], [key#179, value#180, topic#181, partition#182, offset#183L, timestamp#184, timestampType#185], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@59d8c1ad,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:9093, subscribe -> reviews, startingOffsets -> latest),None), kafka, [key#172, value#173, topic#174, partition#175, offset#176L, timestamp#177, timestampType#178]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, IntegerType\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from time import sleep\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"StreamingReviews\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "#Review_ID,Movie_ID,Reviewer_Name,Review_Rating,Reviewer_Nationality,Reviewer_Age,Review_Date,Sex\n",
    "dataSchema = StructType(\n",
    "    [StructField(\"Review_ID\", StringType(), True),\n",
    "     StructField(\"Movie_ID\", StringType(), True),\n",
    "     StructField(\"Reviewer_Name\", StringType(), True),\n",
    "     StructField(\"Review_Rating\", IntegerType(), True),\n",
    "     StructField(\"Reviewer_Nationality\", StringType(), True),\n",
    "     StructField(\"Reviewer_Age\", IntegerType(), True),\n",
    "     StructField(\"Review_Date\", StringType(), True),\n",
    "     StructField(\"Sex\", StringType(), True)\n",
    "     ])\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# We need to set the following configuration whenever we need to use GCS.\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\n",
    "    \"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\"\n",
    ")\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"temp_de2024_2069997\"\n",
    "spark.conf.set(\"temporaryGcsBucket\", bucket)\n",
    "\n",
    "# Read the whole dataset as a batch\n",
    "kafkaStream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9093\") \\\n",
    "    .option(\"subscribe\", \"reviews\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "df = kafkaStream.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "df1 = df.select(from_json(df.value, dataSchema.simpleString()))\n",
    "\n",
    "df1.printSchema()\n",
    "\n",
    "sdf = df1.select(col(\"from_json(value).*\"))\n",
    "\n",
    "# Converting releaseDateTheaters to datetime\n",
    "sdf = sdf.withColumn(\n",
    "    \"Review_Date\", to_date(col(\"Review_Date\"), \"M/d/yyyy\")\n",
    ")\n",
    "\n",
    "sdf.printSchema()\n",
    "\n",
    "# Step 1: Calculate the daily average rating for each movie\n",
    "daily_movie_ratings = (\n",
    "    sdf\n",
    "    .groupBy(\"Movie_ID\", \"Review_Date\")\n",
    "    .agg(avg(\"Review_Rating\").alias(\"Daily_Average_Rating\"))\n",
    ")\n",
    "\n",
    "# Step 2: Filter movies with very low average ratings\n",
    "low_rated_movies = (\n",
    "    daily_movie_ratings\n",
    "    .filter(col(\"Daily_Average_Rating\") < 4.0)  # Threshold for low ratings\n",
    ")\n",
    "\n",
    "def my_foreach_batch_function(df, batch_id):\n",
    "   # Saving the data to BigQuery as batch processing sink -see, use write(), save(), etc.\n",
    "    df.write.format('bigquery') \\\n",
    "      .option('table', 'dataengineering-439112.labdataset.low_rated_movies') \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .save()\n",
    "\n",
    "# Output to the console or sink\n",
    "query = (\n",
    "    low_rated_movies\n",
    "    .writeStream\n",
    "    .outputMode(\"complete\") \n",
    "    .trigger(processingTime=\"2 seconds\")\n",
    "    .foreachBatch(my_foreach_batch_function)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cea98d1-39ae-434e-9566-e5e3618e1921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
