{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import col, avg, explode, split, dense_rank, row_number, countDistinct, sum, stddev, rank, regexp_replace, floor, trim, udf\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, Tokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Configuration\n",
    "sparkConf = SparkConf()\n",
    "#sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"IMDB_Analysis\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Set up Hadoop configuration for Google Cloud Storage\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Variables and Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BigQuery dataset and table\n",
    "project_id = \"dataengineering-439112\"\n",
    "dataset_table = \"labdataset.imdb_top_1000\"\n",
    "output_table_decade_trends = \"labdataset.imdb_decade_trends\"\n",
    "output_table_genre_ratings = \"labdataset.imdb_genre_ratings\"\n",
    "bucket_name = \"imdb-output-bucket\"\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IMDB_Analysis\") \\\n",
    "    .config(\"temporaryGcsBucket\", bucket_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data\n",
    "imdb_df = spark.read.format(\"bigquery\") \\\n",
    "    .option(\"project\", project_id) \\\n",
    "    .option(\"table\", dataset_table) \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on Dataset based on Decades and on Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of Decade-Based Trends:\n",
      "+------+-----------------+-------------------+------------+------------------+------------------+\n",
      "|Decade|Avg_IMDB_Rating  |StdDev_IMDB_Rating |Movies_Count|Avg_Runtime       |StdDev_Runtime    |\n",
      "+------+-----------------+-------------------+------------+------------------+------------------+\n",
      "|1920  |8.127272727272727|0.11621744062959531|11          |86.27272727272727 |27.699375647393413|\n",
      "|1930  |7.966666666666668|0.2234481550923712 |24          |102.125           |34.77014187427838 |\n",
      "|1940  |8.025714285714283|0.22242778622019257|35          |109.8             |17.75485236797805 |\n",
      "|1950  |8.05892857142857 |0.25452785168705466|56          |118.67857142857143|31.333778304023   |\n",
      "|1960  |7.973972602739731|0.2546274613465714 |73          |126.45205479452055|31.549869050885317|\n",
      "+------+-----------------+-------------------+------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Example of Ratings by Genre:\n",
      "+----------------+-----------------+-------------------+------------+------------------+--------------------+\n",
      "|Individual_Genre|Avg_IMDB_Rating  |StdDev_IMDB_Rating |Movies_Count|Avg_Runtime       |Avg_Box_Office      |\n",
      "+----------------+-----------------+-------------------+------------+------------------+--------------------+\n",
      "|Crime           |7.952884615384611|0.30738313619841784|208         |123.26442307692308|3.3443768879807692E7|\n",
      "|Romance         |7.925600000000003|0.2368829845875317 |125         |119.928           |3.4547173152E7      |\n",
      "|Thriller        |7.909489051094895|0.2623096541468723 |137         |119.64233576642336|4.711261993430657E7 |\n",
      "|Adventure       |7.953846153846153|0.2817893432004065 |195         |125.42564102564103|1.4359220223589742E8|\n",
      "|Drama           |7.959418282548479|0.27983786587428466|722         |126.6371191135734 |3.7844577141274236E7|\n",
      "+----------------+-----------------+-------------------+------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean and preprocess data\n",
    "imdb_df = imdb_df.withColumn(\"Runtime\", regexp_replace(col(\"Runtime\"), \" min\", \"\").cast(\"int\"))\n",
    "imdb_df = imdb_df.withColumn(\"Gross\", col(\"Gross\").cast(\"double\"))\n",
    "\n",
    "# Fill missing values\n",
    "imdb_df = imdb_df.fillna({\n",
    "    \"Runtime\": 0,\n",
    "    \"IMDB_Rating\": 0,\n",
    "    \"No_of_Votes\": 0,\n",
    "    \"Gross\": 0,\n",
    "    \"Director\": \"unknown\",\n",
    "    \"Genre\": \"unknown\",\n",
    "    \"Overview\": \"unknown\"\n",
    "})\n",
    "\n",
    "# Add Decade column\n",
    "imdb_df = imdb_df.withColumn(\"Decade\", (floor(col(\"Released_Year\").cast(\"int\") / 10) * 10).cast(\"int\"))\n",
    "\n",
    "# Split genres into individual rows and remove duplicates\n",
    "split_genres_df = imdb_df.withColumn(\"Individual_Genre\", explode(split(trim(col(\"Genre\")), \", \"))).dropDuplicates([\"Individual_Genre\", \"Series_Title\"])\n",
    "\n",
    "# Decade-Based Trends\n",
    "decade_trends = imdb_df.groupBy(\"Decade\") \\\n",
    "    .agg(\n",
    "        avg(\"IMDB_Rating\").alias(\"Avg_IMDB_Rating\"),\n",
    "        stddev(\"IMDB_Rating\").alias(\"StdDev_IMDB_Rating\"),\n",
    "        countDistinct(\"Series_Title\").alias(\"Movies_Count\"),\n",
    "        avg(\"Runtime\").alias(\"Avg_Runtime\"),\n",
    "        stddev(\"Runtime\").alias(\"StdDev_Runtime\")\n",
    "    ) \\\n",
    "    .orderBy(\"Decade\")\n",
    "\n",
    "# Display an example of Decade Trends DataFrame\n",
    "print(\"Example of Decade-Based Trends:\")\n",
    "decade_trends.show(5, truncate=False)\n",
    "\n",
    "# Write decade trends to BigQuery\n",
    "decade_trends.write.format(\"bigquery\") \\\n",
    "    .option(\"table\", f\"{project_id}.{output_table_decade_trends}\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# Ratings, Runtime, and Movies Count by Genre \n",
    "avg_rating_by_genre = split_genres_df.groupBy(\"Individual_Genre\") \\\n",
    "    .agg(\n",
    "        avg(\"IMDB_Rating\").alias(\"Avg_IMDB_Rating\"),\n",
    "        stddev(\"IMDB_Rating\").alias(\"StdDev_IMDB_Rating\"),\n",
    "        countDistinct(\"Series_Title\").alias(\"Movies_Count\"),\n",
    "        avg(\"Runtime\").alias(\"Avg_Runtime\"),\n",
    "        avg(\"Gross\").alias(\"Avg_Box_Office\")\n",
    "    )\n",
    "\n",
    "# Display an example of Genre Ratings DataFrame\n",
    "print(\"Example of Ratings by Genre:\")\n",
    "avg_rating_by_genre.show(5, truncate=False)\n",
    "\n",
    "# Write genre ratings to BigQuery\n",
    "avg_rating_by_genre.write.format(\"bigquery\") \\\n",
    "    .option(\"table\", f\"{project_id}.{output_table_genre_ratings}\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Poster_Link='https://m.media-amazon.com/images/M/MV5BMjRjMTYwMTYtMmRkNi00MmVkLWE0MjQtNmM3YjI0NWFhZDNmXkEyXkFqcGdeQXVyODE5NzE3OTE@._V1_UX67_CR0,0,67,98_AL_.jpg', Series_Title='Dil Chahta Hai', Released_Year=2001, Certificate='Unrated', Runtime=183, Genre='Comedy, Drama, Romance', IMDB_Rating=8.1, Overview='Three inseparable childhood friends are just out of college. Nothing comes between them - until they each fall in love, and their wildly different approaches to relationships creates tension.', Meta_score=None, Director='Farhan Akhtar', Star1='Aamir Khan', Star2='Saif Ali Khan', Star3='Akshaye Khanna', Star4='Preity Zinta', No_of_Votes=66803, Gross=300000.0, Decade=2000)\n"
     ]
    }
   ],
   "source": [
    "print(imdb_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling + Additional Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IMDB_Analysis\") \\\n",
    "    .config(\"temporaryGcsBucket\", \"imdb-output-bucket\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load original data from BigQuery\n",
    "imdb_df = spark.read.format(\"bigquery\") \\\n",
    "    .option(\"project\", \"dataengineering-439112\") \\\n",
    "    .option(\"table\", \"labdataset.imdb_top_1000\") \\\n",
    "    .load()\n",
    "\n",
    "# Load the additional dataset from Cloud Storage\n",
    "additional_df = spark.read.csv(\"gs://imdb-bucket-data/imdb_additional_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Deduplicate additional_df before the join\n",
    "additional_df = additional_df.dropDuplicates([\"Series_Title\"])\n",
    "\n",
    "# Perform the join\n",
    "imdb_df = imdb_df.join(additional_df, on=\"Series_Title\", how=\"left\")\n",
    "\n",
    "# Remove duplicates after the join\n",
    "imdb_df = imdb_df.dropDuplicates()\n",
    "\n",
    "# Drop rows with missing critical data\n",
    "imdb_df = imdb_df.dropna(subset=[\"IMDB_Rating\", \"Gross\", \"Runtime\", \"Director\", \"Genre\", \"Overview\"])\n",
    "\n",
    "# Check row count\n",
    "print(f\"Final row count: {imdb_df.count()}\")\n",
    "\n",
    "# Clean and preprocess data\n",
    "imdb_df = imdb_df.withColumn(\"Runtime\", regexp_replace(col(\"Runtime\"), \" min\", \"\").cast(\"int\"))\n",
    "imdb_df = imdb_df.withColumn(\"Gross\", col(\"Gross\").cast(\"double\"))\n",
    "\n",
    "# Remove rows with missing values \n",
    "imdb_df = imdb_df.dropna(subset=[\"IMDB_Rating\", \"Gross\", \"Runtime\", \"Director\", \"Genre\", \"Overview\"])\n",
    "\n",
    "# Print the count of remaining rows\n",
    "print(f\"Number of rows after dropping missing values: {imdb_df.count()}\")\n",
    "\n",
    "# Add Decade column\n",
    "imdb_df = imdb_df.withColumn(\"Decade\", (floor(col(\"Released_Year\").cast(\"int\") / 10) * 10).cast(\"int\"))\n",
    "\n",
    "# Preprocess data\n",
    "# Tokenize Overview text\n",
    "tokenizer = Tokenizer(inputCol=\"Overview\", outputCol=\"Tokenized_Overview\")\n",
    "imdb_df = tokenizer.transform(imdb_df)\n",
    "\n",
    "# Remove stopwords\n",
    "remover = StopWordsRemover(inputCol=\"Tokenized_Overview\", outputCol=\"Filtered_Overview\")\n",
    "imdb_df = remover.transform(imdb_df)\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = CountVectorizer(inputCol=\"Filtered_Overview\", outputCol=\"Overview_Features\")\n",
    "vectorizer_model = vectorizer.fit(imdb_df)\n",
    "imdb_df = vectorizer_model.transform(imdb_df)\n",
    "\n",
    "# Train LDA Model\n",
    "lda = LDA(k=5, maxIter=10, featuresCol=\"Overview_Features\", seed=42)\n",
    "lda_model = lda.fit(imdb_df)\n",
    "imdb_df = lda_model.transform(imdb_df)\n",
    "\n",
    "# Features\n",
    "# Index and encode categorical features\n",
    "genre_indexer = StringIndexer(inputCol=\"Genre\", outputCol=\"Genre_Indexed\", handleInvalid=\"keep\")\n",
    "genre_encoder = OneHotEncoder(inputCol=\"Genre_Indexed\", outputCol=\"Genre_Encoded\")\n",
    "\n",
    "director_indexer = StringIndexer(inputCol=\"Director\", outputCol=\"Director_Indexed\", handleInvalid=\"keep\")\n",
    "director_encoder = OneHotEncoder(inputCol=\"Director_Indexed\", outputCol=\"Director_Encoded\")\n",
    "\n",
    "location_indexer = StringIndexer(inputCol=\"Filming_Locations\", outputCol=\"Location_Indexed\", handleInvalid=\"keep\")\n",
    "location_encoder = OneHotEncoder(inputCol=\"Location_Indexed\", outputCol=\"Location_Encoded\")\n",
    "\n",
    "# Assemble all features, including topics and additional data\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"IMDB_Rating\", \"Runtime\", \"Awards_Won\", \"Oscars_Nominated\", \n",
    "        \"Production_Budget\", \"Audience_Score\", \"Genre_Encoded\", \n",
    "        \"Director_Encoded\", \"Location_Encoded\", \"topicDistribution\"\n",
    "    ],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Gross\", numTrees=50, maxDepth=10, seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    genre_indexer, genre_encoder, director_indexer, director_encoder, \n",
    "    location_indexer, location_encoder, assembler, rf\n",
    "])\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df = imdb_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Calculate R²\n",
    "evaluator = RegressionEvaluator(labelCol=\"Gross\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print(f\"R²: {r2}\")\n",
    "\n",
    "# Feature importances\n",
    "rf_model = model.stages[-1]  # RandomForestRegressor\n",
    "importances = rf_model.featureImportances.toArray()\n",
    "\n",
    "# Map feature importances\n",
    "assembler_inputs = assembler.getInputCols()\n",
    "feature_importance_map = dict(zip(assembler_inputs, importances))\n",
    "\n",
    "# Sort and print feature importances\n",
    "sorted_importance = sorted(feature_importance_map.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Feature Importances:\")\n",
    "for feature, score in sorted_importance:\n",
    "    print(f\"{feature}: {score}\")\n",
    "\n",
    "# Convert feature importances to a DataFrame\n",
    "feature_importances_df = spark.createDataFrame(\n",
    "    [(k, float(v)) for k, v in feature_importance_map.items()],\n",
    "    [\"Feature\", \"Importance\"]\n",
    ")\n",
    "\n",
    "# Save to BigQuery\n",
    "project_id = \"dataengineering-439112\"\n",
    "output_table_feature_importances = \"labdataset.imdb_feature_importances\"\n",
    "\n",
    "feature_importances_df.write.format(\"bigquery\") \\\n",
    "    .option(\"table\", f\"{project_id}.{output_table_feature_importances}\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Feature importances successfully saved to BigQuery.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
